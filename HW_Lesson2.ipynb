{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "import gc\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from datetime import datetime\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from razdel import tokenize\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, precision_recall_curve, confusion_matrix\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#источник данных: https://www.kaggle.com/austinreese/goodreads-books\n",
    "books = pd.read_csv('goodreads_books.csv', quotechar='\\\"', \n",
    "                 escapechar='\\\\', \n",
    "                 error_bad_lines=False, nrows=20000)\n",
    "\n",
    "#books.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>series</th>\n",
       "      <th>cover_link</th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>review_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>asin</th>\n",
       "      <th>settings</th>\n",
       "      <th>characters</th>\n",
       "      <th>awards</th>\n",
       "      <th>amazon_redirect_link</th>\n",
       "      <th>worldcat_redirect_link</th>\n",
       "      <th>recommended_books</th>\n",
       "      <th>books_in_series</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630104</td>\n",
       "      <td>Inner Circle</td>\n",
       "      <td>https://www.goodreads.com//book/show/630104.In...</td>\n",
       "      <td>(Private #5)</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>Kate Brian, Julian Peploe</td>\n",
       "      <td>https://www.goodreads.com/author/show/94091.Ka...</td>\n",
       "      <td>7597</td>\n",
       "      <td>196</td>\n",
       "      <td>4.03</td>\n",
       "      <td>...</td>\n",
       "      <td>9781416950417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.goodreads.com//book_link/follow/17...</td>\n",
       "      <td>https://www.goodreads.com//book_link/follow/8?...</td>\n",
       "      <td>726458, 726458, 1537534, 3047848, 1651302, 304...</td>\n",
       "      <td>381489, 381501, 352428, 630103, 1783281, 17832...</td>\n",
       "      <td>Reed Brennan arrived at Easton Academy expecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9487</td>\n",
       "      <td>A Time to Embrace</td>\n",
       "      <td>https://www.goodreads.com//book/show/9487.A_Ti...</td>\n",
       "      <td>(Timeless Love #2)</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>Karen Kingsbury</td>\n",
       "      <td>https://www.goodreads.com/author/show/3159984....</td>\n",
       "      <td>4179</td>\n",
       "      <td>177</td>\n",
       "      <td>4.35</td>\n",
       "      <td>...</td>\n",
       "      <td>9781595542328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.goodreads.com//book_link/follow/17...</td>\n",
       "      <td>https://www.goodreads.com//book_link/follow/8?...</td>\n",
       "      <td>127352, 127352, 40642197, 127353, 127354, 3891...</td>\n",
       "      <td>115036</td>\n",
       "      <td>Ideje az Ã¶lelÃ©snek TÃ¶rtÃ©net a remÃ©nyrÅl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6050894</td>\n",
       "      <td>Take Two</td>\n",
       "      <td>https://www.goodreads.com//book/show/6050894-t...</td>\n",
       "      <td>(Above the Line #2)</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>Karen Kingsbury</td>\n",
       "      <td>https://www.goodreads.com/author/show/3159984....</td>\n",
       "      <td>6288</td>\n",
       "      <td>218</td>\n",
       "      <td>4.23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bloomington, Indiana(United States)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.goodreads.com//book_link/follow/17...</td>\n",
       "      <td>https://www.goodreads.com//book_link/follow/8?...</td>\n",
       "      <td>706250, 706250, 666481, 11942636, 706241, 1273...</td>\n",
       "      <td>4010795, 40792877, 7306261</td>\n",
       "      <td>Filmmakers Chase Ryan and Keith Ellison have c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id              title  \\\n",
       "0   630104       Inner Circle   \n",
       "1     9487  A Time to Embrace   \n",
       "2  6050894           Take Two   \n",
       "\n",
       "                                                link               series  \\\n",
       "0  https://www.goodreads.com//book/show/630104.In...         (Private #5)   \n",
       "1  https://www.goodreads.com//book/show/9487.A_Ti...   (Timeless Love #2)   \n",
       "2  https://www.goodreads.com//book/show/6050894-t...  (Above the Line #2)   \n",
       "\n",
       "                                          cover_link  \\\n",
       "0  https://i.gr-assets.com/images/S/compressed.ph...   \n",
       "1  https://i.gr-assets.com/images/S/compressed.ph...   \n",
       "2  https://i.gr-assets.com/images/S/compressed.ph...   \n",
       "\n",
       "                      author  \\\n",
       "0  Kate Brian, Julian Peploe   \n",
       "1            Karen Kingsbury   \n",
       "2            Karen Kingsbury   \n",
       "\n",
       "                                         author_link  rating_count  \\\n",
       "0  https://www.goodreads.com/author/show/94091.Ka...          7597   \n",
       "1  https://www.goodreads.com/author/show/3159984....          4179   \n",
       "2  https://www.goodreads.com/author/show/3159984....          6288   \n",
       "\n",
       "   review_count  average_rating  ...         isbn13  asin  \\\n",
       "0           196            4.03  ...  9781416950417   NaN   \n",
       "1           177            4.35  ...  9781595542328   NaN   \n",
       "2           218            4.23  ...            NaN   NaN   \n",
       "\n",
       "                              settings  characters  awards  \\\n",
       "0                                  NaN         NaN     NaN   \n",
       "1                                  NaN         NaN     NaN   \n",
       "2  Bloomington, Indiana(United States)         NaN     NaN   \n",
       "\n",
       "                                amazon_redirect_link  \\\n",
       "0  https://www.goodreads.com//book_link/follow/17...   \n",
       "1  https://www.goodreads.com//book_link/follow/17...   \n",
       "2  https://www.goodreads.com//book_link/follow/17...   \n",
       "\n",
       "                              worldcat_redirect_link  \\\n",
       "0  https://www.goodreads.com//book_link/follow/8?...   \n",
       "1  https://www.goodreads.com//book_link/follow/8?...   \n",
       "2  https://www.goodreads.com//book_link/follow/8?...   \n",
       "\n",
       "                                   recommended_books  \\\n",
       "0  726458, 726458, 1537534, 3047848, 1651302, 304...   \n",
       "1  127352, 127352, 40642197, 127353, 127354, 3891...   \n",
       "2  706250, 706250, 666481, 11942636, 706241, 1273...   \n",
       "\n",
       "                                     books_in_series  \\\n",
       "0  381489, 381501, 352428, 630103, 1783281, 17832...   \n",
       "1                                             115036   \n",
       "2                         4010795, 40792877, 7306261   \n",
       "\n",
       "                                         description  \n",
       "0  Reed Brennan arrived at Easton Academy expecti...  \n",
       "1  Ideje az Ã¶lelÃ©snek TÃ¶rtÃ©net a remÃ©nyrÅl,...  \n",
       "2  Filmmakers Chase Ryan and Keith Ellison have c...  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19019"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = books[~books['description'].isnull()][['genre_and_votes', 'description']]\n",
    "books['book_id'] = range(len(books))\n",
    "books.index = range(len(books))\n",
    "#books.head(3)\n",
    "len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18628"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#удаляем все книги с описанием меньшем, чем в 100 символов:\n",
    "\n",
    "i = 0\n",
    "for d in books[\"description\"]:\n",
    "    if len(d) < 100:\n",
    "        books.drop([i], inplace=True)\n",
    "    i+=1\n",
    "len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#обновляем индексы:\n",
    "\n",
    "books = books[~books['description'].isnull()][['genre_and_votes', 'description']]\n",
    "books['book_id'] = range(len(books))\n",
    "books.index = range(len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15945"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#удаляем пустые строки и строки, содержащие \"плохие\" (нечитабельные) символы\n",
    "\n",
    "i = 0\n",
    "for d in books[\"description\"]:\n",
    "    if not bool(re.match('^(.[a-zA-Z])', d)):\n",
    "        books.drop([i], inplace=True)\n",
    "    i+=1\n",
    "len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Young', 'Christian', 'Economics-Finance', 'Romance', 'Fiction',\n",
       "       'Sequential', 'Fantasy', 'Mystery', 'Art', 'Nonfiction', 'History',\n",
       "       'Historical-Historical', 'Horror', 'Fantasy-Paranormal', nan,\n",
       "       'Romance-Paranormal', 'World', 'Plays', 'Thriller', 'Short',\n",
       "       'Romance-Romantic', 'Childrens-Picture', 'Classics',\n",
       "       'Erotica-BDSM', 'Womens', 'Fantasy-Urban', 'New', 'Business',\n",
       "       'Science', 'Childrens-Middle', 'Paranormal-Vampires', 'Childrens',\n",
       "       'Cultural-Africa', 'Religion', 'Autobiography-Memoir',\n",
       "       'Romance-Historical', 'Poetry', 'Dark', 'Novels', 'Philosophy',\n",
       "       'LGBT', 'Romance-M', 'Economics', 'Politics', 'Humor',\n",
       "       'Culture-Film', 'Self', 'Food', 'Adult', 'Sports-Sports',\n",
       "       'Mystery-Crime', 'Media', 'Music', 'Writing-Essays', 'LGBT-Gay',\n",
       "       'Fairy', 'Fantasy-Magic', 'Horror-Zombies', 'Cultural-Japan',\n",
       "       'Animals', 'Inspirational', 'Romance-Erotic', 'Paranormal-Witches',\n",
       "       'Religion-Theology', 'Health-Mental', 'Contemporary',\n",
       "       'Romance-Science', 'Psychology', 'Literature',\n",
       "       'Pseudoscience-Numerology', 'Health', 'Literature-Banned',\n",
       "       'Fantasy-Mythology', 'War-Military', 'Race', 'Religion-Islam',\n",
       "       'Spirituality', 'Religion-Baha', 'Computer', 'Cultural-France',\n",
       "       'Language-Writing', 'Paranormal', 'Paranormal-Shapeshifters',\n",
       "       'Biography', 'Travel', 'Erotica-Menage', 'Design',\n",
       "       'Paranormal-Angels', 'Cultural-Russia', 'Polyamorous-Reverse',\n",
       "       'Fantasy-Dark', 'Thriller-Mystery', 'Parenting', 'European',\n",
       "       'Westerns', 'Epic', 'Religion-Buddhism', 'Category',\n",
       "       'Romance-Contemporary', 'Leadership', 'Novels-Light',\n",
       "       'Sports-Mountaineering', 'Cultural-Australia', 'Romance-Christian',\n",
       "       'Urban', 'Reference', 'Feminism', 'Sociology-Abuse',\n",
       "       'Holiday-Christmas', 'Environment-Nature', 'Realistic',\n",
       "       'Cultural-African', 'Modern', 'Adventure', 'Crime-True',\n",
       "       'Cultural-India', 'Lds-Lds', 'Anthologies', 'Animals-Horses',\n",
       "       'Mythology-Mermaids', 'Asian', 'Mystery-Noir', 'Marriage',\n",
       "       'Science-Mathematics', 'Drama', 'Cultural-China', 'Combat-Martial',\n",
       "       'Amish', 'Esoterica-Astrology', 'Roman', 'Occult', 'Dungeons',\n",
       "       'Sports-Baseball', 'Business-Amazon', 'Cultural-Pakistan',\n",
       "       'Writing-Books', 'Historical', 'Cultural-Canada', 'Family',\n",
       "       'Adventure-Pirates', 'Military', 'Health-Medicine', 'Love',\n",
       "       'Art-Photography', 'GLBT-Lesbian', 'Cultural-Brazil',\n",
       "       'Science-Physics', 'Mystery-Detective', 'Education',\n",
       "       'Christian-Christian', 'Paranormal-Ghosts', 'American-Southern',\n",
       "       'Religion-Christianity', 'Childrens-Chapter', 'Aviation',\n",
       "       'Sexuality', 'Adventure-Survival', 'Literature-American',\n",
       "       'Cultural-Portugal', 'Sociology', 'Anthologies-Collections',\n",
       "       'Religion-Faith', 'Fantasy-Dragons', 'Romance-Love',\n",
       "       'Relationships', 'Mental', 'War', 'Kids', 'Apocalyptic-Post',\n",
       "       'Literature-European', 'Couture-Fashion', 'Prayer',\n",
       "       'Language-Communication', 'Science-Engineering',\n",
       "       'Comics-Superheroes', 'Architecture', 'Suspense', 'Diary-Journal',\n",
       "       'Unfinished', 'Textbooks', 'Paranormal-Demons', 'Sports-Hockey',\n",
       "       'Anthropology', 'Business-Management', 'Fantasy-High',\n",
       "       'Mystery-Cozy', 'Cultural-Italy', 'Fan', 'Sci',\n",
       "       'Esoterica-Alchemy', 'Currency-Money', 'Social', 'Culture-Society',\n",
       "       'Environment', 'Central', 'Religion-Paganism', 'Book',\n",
       "       'Christianity-Catholic', 'Religion-Judaism', 'Own', 'History-1917',\n",
       "       'Psychology-Counselling', 'Shapeshifters-Werewolves',\n",
       "       'Historical-Medieval', 'Occult-Mysticism', 'GLBT-Queer',\n",
       "       'Manga-Yaoi', 'Childrens-Juvenile', 'Audiobook', 'Artificial',\n",
       "       'Speculative', 'Medical', 'Computers-Internet',\n",
       "       'Travel-Travelogue', 'History-World', 'Crafts-Quilting',\n",
       "       'History-Ancient', 'Sports-Cycling', 'Crafts-Sewing', 'African',\n",
       "       'Romance-African', 'Culture-Pop', 'Classics-Cult',\n",
       "       'Romance-Interracial', 'Pop', 'Thriller-Psychological',\n",
       "       'Nurses-Nursing', 'Witchcraft', 'War-World', 'Games-Chess',\n",
       "       'Literature-Marathi', 'Literature-Asian', 'Criticism-Literary',\n",
       "       'Futuristic', 'Sports-Fitness', 'Animals-Cats', 'Literature-Black',\n",
       "       'Cultural-Germany', 'Literature-21st', 'Academic-Read',\n",
       "       'Cultural-Israel', 'Games-Role', 'Parenting-Adoption', 'Law',\n",
       "       'Childrens-Young', 'Eastern', 'Warfare-Fighters', 'Action',\n",
       "       'Cultural-Ireland', 'Biology-Neuroscience', 'Romance-Lesbian',\n",
       "       'Animals-Birds', 'Polyamory', 'Science-Computer', 'Magical',\n",
       "       'Gothic', 'Football', 'Gay', 'Alcohol-Beer', 'Philosophy-Theory',\n",
       "       'Pseudoscience-Cryptozoology', 'Academic-School', 'Productivity',\n",
       "       'Business-Entrepreneurship', 'Horror-Ghost', 'Plays-Theatre',\n",
       "       'Cultural-Spain', 'Spirituality-New', 'Buisness',\n",
       "       'Classics-Modern', 'Pseudoscience-Conspiracy',\n",
       "       'Harlequin-Harlequin', 'Fantasy-Supernatural', 'Space',\n",
       "       'Gender-Gender', 'Religion-Church', 'Religion-Esoterica',\n",
       "       'Cultural-Turkish', 'LGBT-Asexual', 'Nobel', 'Biology-Ecology',\n",
       "       'Biblical', 'Science-Technology', 'Occult-Magick'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['genre_and_votes'] = books['genre_and_votes'].str.split(' ').str[0]\n",
    "\n",
    "books['genre_and_votes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#так как длина books изменилась, снова обновляем индексы:\n",
    "\n",
    "books = books[~books['description'].isnull()][['genre_and_votes', 'description']]\n",
    "books['book_id'] = range(len(books))\n",
    "books.index = range(len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bea435086b2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbooks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"description\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetect_langs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#print(d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\langdetect\\detector_factory.py\u001b[0m in \u001b[0;36mdetect_langs\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_factory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\langdetect\\detector.py\u001b[0m in \u001b[0;36mget_probabilities\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlangprob\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_detect_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlangprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\langdetect\\detector.py\u001b[0m in \u001b[0;36m_detect_block\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_lang_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_normalize_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONV_THRESHOLD\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_LIMIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\langdetect\\detector.py\u001b[0m in \u001b[0;36m_update_lang_prob\u001b[1;34m(self, prob, word, alpha)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBASE_FREQ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             \u001b[0mprob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlang_prob_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# удаляем все неанглоязычные описания (чтобы темы в дальнейшем были угадываемыми)\n",
    "\n",
    "i = 0\n",
    "for d in books[\"description\"]:\n",
    "    #print(i)\n",
    "    lang = str(detect_langs(d))\n",
    "    if not bool(re.search(\"en\", lang)):\n",
    "        #print(d)\n",
    "        books.drop([i], inplace=True)\n",
    "    elif int(lang[lang.index('en') + 5]) < 8:\n",
    "        books.drop([i], inplace=True)        \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d='Just a few months shy of her 30th birthday, Gus Curtis finally feels like she has it all: a strong career, great friends, and a wonderful boyfriend. But all of this comes crashing down when Gus discovers Nate, her \"Mr. Right,\" hooking up behind her back with her so-called \"friend\" Helen. Soon it seems like the life Gus has worked to make so adult looks a lot like the one s Just a few months shy of her 30th birthday, Gus Curtis finally feels like she has it all: a strong career, great friends, and a wonderful boyfriend. But all of this comes crashing down when Gus discovers Nate, her \"Mr. Right,\" hooking up behind her back with her so-called \"friend\" Helen. Soon it seems like the life Gus has worked to make so adult looks a lot like the one she already had as a teenager, and Gus is left with more questions than answers: Can she win Nate back before she turns 30 alone? (And if so, does she really want him?) Is Helen really as devious and manipulative as she seems, or, worse, is Gus more like her frenemy than she ever imagined? And is she ever going to grow up? With the clock ticking down to her birthday, Gus discovers that sometimes the best thing about best-laid plans is trashing them altogether.'\n",
    "#lang = str(detect_langs(d))\n",
    "#print(not bool(re.search(\"en\", str(lang))))\n",
    "#i = lang.index('en')\n",
    "#print(int(lang[i+5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(books[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#снова обновляем индексы\n",
    "\n",
    "books = books[~books['description'].isnull()][['genre_and_votes', 'description']]\n",
    "books['book_id'] = range(len(books))\n",
    "books.index = range(len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#меняем название колонки \"genre_and_votes\", так как в ней теперь только один жанр\n",
    "\n",
    "books = books.rename(columns={\"genre_and_votes\": \"genre\"})\n",
    "books = books[books['genre'].notna()]\n",
    "print(len(books))\n",
    "books.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[\"genre\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выбираем наиболее популярные жанры:\n",
    "\n",
    "books = books[books.groupby('genre').genre.transform('count') > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[['book_id', 'description']].to_csv(\"books_descriptions.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\"u10{}\".format(i) for i in range(1000, 8000)]\n",
    "users = pd.DataFrame({'uid': users, 'churn': [0 for i in range(6000)]+[1 for i in range(1000)]})\n",
    "users.sample(frac=1).to_csv(\"users_churn.csv\", index=None)\n",
    "users.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction = books[books['genre']==u'Fiction']['book_id'].values\n",
    "fantasy = books[books['genre']==u'Fantasy']['book_id'].values\n",
    "romance = books[books['genre']==u'Romance']['book_id'].values\n",
    "young = books[books['genre']==u'Young']['book_id'].values\n",
    "nonfiction = books[books['genre']==u'Nonfiction']['book_id'].values\n",
    "historical = books[books['genre']==u'Historical-Historical']['book_id'].values\n",
    "science = books[books['genre']==u'Science']['book_id'].values\n",
    "mystery = books[books['genre']==u'Mystery']['book_id'].values\n",
    "#sequential = books[books['genre']==u'Sequential']['book_id'].values\n",
    "#history = books[books['genre']==u'History']['book_id'].values\n",
    "#poetry = books[books['genre']==u'Poetry']['book_id'].values\n",
    "#horror = books[books['genre']==u'Horror']['book_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for i in range(2000):\n",
    "    q = np.random.choice(young, 5).tolist()+np.random.choice(mystery, 1).tolist()\n",
    "    total.append(q)\n",
    "for i in range(2000):\n",
    "    q = np.random.choice(fantasy, 5).tolist()+np.random.choice(young, 1).tolist()\n",
    "    total.append(q)\n",
    "for i in range(1000):\n",
    "    q = np.random.choice(historical, 5).tolist()+np.random.choice(science, 1).tolist()\n",
    "    total.append(q)\n",
    "for i in range(1000):\n",
    "    q = np.random.choice(mystery, 5).tolist()+np.random.choice(historical, 1).tolist()\n",
    "    total.append(q)\n",
    "for i in range(1000):\n",
    "    q = np.random.choice(nonfiction, 5).tolist()+np.random.choice(romance, 1).tolist()\n",
    "    total.append(q)\n",
    "#for i in range(1000):\n",
    "#    q = np.random.choice(sequential, 5).tolist()+np.random.choice(historical, 1).tolist()\n",
    "#    total.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'uid': [\"u10{}\".format(i) for i in range(1000, 8000)], \n",
    "              'books_descriptions': total}).sample(frac=1).to_csv(\"users_books.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[books['book_id']==492909]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'uid': [\"u10{}\".format(i) for i in range(1000, 8000)], \n",
    "              'books_descriptions': total}).sample(frac=1).to_csv(\"users_books.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_en = stopwords.words('english')\n",
    "print(len(stopword_en))\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopword_en += additional_stopwords\n",
    "len(stopword_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"books_descriptions.csv\")\n",
    "print(books.shape)\n",
    "books.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_en] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста. Будет долго...\n",
    "books['description'] = books['description'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста. Будет очень долго...\n",
    "books['description'] = books['description'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in books['description'].values]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_dictionary[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=25, id2word=common_dictionary)#, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in books['description'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=lda.show_topics(num_topics=25, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = news['title'].iloc[0]\n",
    "\n",
    "def get_lda_vector(text):\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in books['description'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix['book_id'] = books['book_id'].values\n",
    "topic_matrix = topic_matrix[['book_id']+['topic_{}'.format(i) for i in range(25)]]\n",
    "topic_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#обработка пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\"users_books.csv\")\n",
    "users.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict(zip(topic_matrix['book_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(25)]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict[230]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция get_user_embedding, использующая np.mean и результаты модели для неё"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_articles_list = users['books_descriptions'].iloc[33]\n",
    "\n",
    "def get_user_embedding(user_articles_list):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector = np.mean(user_vector, 0)\n",
    "    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_embedding(user_articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['books_descriptions'].iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = pd.DataFrame([i for i in users['books_descriptions'].apply(lambda x: get_user_embedding(x), 1)])\n",
    "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "user_embeddings.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"users_churn.csv\")\n",
    "target.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(user_embeddings, target, 'left')\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(15)]], \n",
    "                                                    X['churn'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "#обучим \n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#наши прогнозы для тестовой выборки\n",
    "preds = logreg.predict_proba(X_test)[:, 1]\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall, F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))\n",
    "\n",
    "thresholds_mean, fscore_mean, precision_mean, recall_mean = thresholds[ix], fscore[ix], precision[ix], recall[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#мы уже нашли ранее \"оптимальный\" порог, когда максимизировали f_score\n",
    "font = {'size' : 15}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, preds>thresholds[ix])\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Non-Churn', 'churn'],\n",
    "                      title='Confusion matrix')\n",
    "plt.savefig(\"conf_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_mean = roc_auc_score(y_test, preds)\n",
    "print(roc_auc_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция get_user_embedding, использующая np.median и результаты модели для неё"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_articles_list = users['books_descriptions'].iloc[33]\n",
    "\n",
    "def get_user_embedding(user_articles_list):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector = np.median(user_vector, 0)\n",
    "    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_embedding(user_articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['books_descriptions'].iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = pd.DataFrame([i for i in users['books_descriptions'].apply(lambda x: get_user_embedding(x), 1)])\n",
    "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "user_embeddings.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"users_churn.csv\")\n",
    "target.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(user_embeddings, target, 'left')\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(15)]], \n",
    "                                                    X['churn'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "#обучим \n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#наши прогнозы для тестовой выборки\n",
    "preds = logreg.predict_proba(X_test)[:, 1]\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall, F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))\n",
    "\n",
    "thresholds_median, fscore_median, precision_median, recall_median = thresholds[ix], fscore[ix], precision[ix], recall[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#font = {'size' : 15}\n",
    "\n",
    "#plt.rc('font', **font)\n",
    "\n",
    "#cnf_matrix = confusion_matrix(y_test, preds>thresholds[ix])\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['Non-Churn', 'churn'],\n",
    "#                      title='Confusion matrix')\n",
    "#plt.savefig(\"conf_matrix.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_median = roc_auc_score(y_test, preds)\n",
    "print(roc_auc_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция get_user_embedding, использующая np.max и результаты модели для неё"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_articles_list = users['books_descriptions'].iloc[33]\n",
    "\n",
    "def get_user_embedding(user_articles_list):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector = np.max(user_vector, 0)\n",
    "    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_embedding(user_articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['books_descriptions'].iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = pd.DataFrame([i for i in users['books_descriptions'].apply(lambda x: get_user_embedding(x), 1)])\n",
    "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "user_embeddings.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"users_churn.csv\")\n",
    "target.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(user_embeddings, target, 'left')\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(15)]], \n",
    "                                                    X['churn'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "#обучим \n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#наши прогнозы для тестовой выборки\n",
    "preds = logreg.predict_proba(X_test)[:, 1]\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall, F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))\n",
    "\n",
    "thresholds_max, fscore_max, precision_max, recall_max = thresholds[ix], fscore[ix], precision[ix], recall[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#font = {'size' : 15}\n",
    "\n",
    "#plt.rc('font', **font)\n",
    "\n",
    "#cnf_matrix = confusion_matrix(y_test, preds>thresholds[ix])\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#plot_confusion_matrix(cnf_matrix, classes=['Non-Churn', 'churn'],\n",
    "#                      title='Confusion matrix')\n",
    "#plt.savefig(\"conf_matrix.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_max = roc_auc_score(y_test, preds)\n",
    "print(roc_auc_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение работы моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean_model: Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds_mean, \n",
    "                                                                                    fscore_mean,\n",
    "                                                                                    precision_mean,\n",
    "                                                                                    recall_mean))\n",
    "print('            Roc_auc=%.3f' % roc_auc_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Median_model: Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds_median, \n",
    "                                                                                    fscore_median,\n",
    "                                                                                    precision_median,\n",
    "                                                                                    recall_median))\n",
    "print('            Roc_auc=%.3f' % roc_auc_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max_model: Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds_max, \n",
    "                                                                                    fscore_max,\n",
    "                                                                                    precision_max,\n",
    "                                                                                    recall_max))\n",
    "print('            Roc_auc=%.3f' % roc_auc_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
